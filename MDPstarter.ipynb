{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "P0 = np.array([[0.5, 0.4, 0.1, 0],\n",
    "               [0.4, 0.5, 0.1, 0],\n",
    "               [0.7, 0.1, 0.1, 0.1],\n",
    "               [0.5, 0.2, 0.2, 0.1]\n",
    "              ])\n",
    "\n",
    "P1 = np.array([[0.7, 0.2, 0.0, 0.1],\n",
    "               [0.2, 0.3, 0.4, 0.1],\n",
    "               [0.5, 0.2, 0.2, 0.1],\n",
    "               [0.4, 0.2, 0.2, 0.2]\n",
    "              ])\n",
    "\n",
    "P2 = np.array([[0.1, 0.3, 0.4, 0.2],\n",
    "               [0.1, 0.3, 0.5, 0.1],\n",
    "               [0.3, 0.3, 0.1, 0.3],\n",
    "               [0.3, 0.4, 0.1, 0.2]\n",
    "              ])\n",
    "\n",
    "R0 = np.array([[1],[3], [5], [12]])\n",
    "\n",
    "R1 = np.array([[0],[2], [4], [11]])\n",
    "\n",
    "R2 = np.array([[-2],[0], [2], [9]])\n",
    "\n",
    "states = [np.array([[1, 0, 0, 0]]), \n",
    "          np.array([[0, 1, 0, 0]]), \n",
    "          np.array([[0, 0, 1, 0]]), \n",
    "          np.array([[0, 0, 0, 1]])]\n",
    "\n",
    "# transition and reward as 'function'  of action\n",
    "# represented as dicts\n",
    "T = {0: P0, 1: P1, 2: P2}\n",
    "R = {0: R0, 1: R1, 2: R2}\n",
    "\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns Q for one state\n",
    "def bellman_backup(s, U, pi, gamma, T=T, R=R, S=states):\n",
    "    action = pi[s]\n",
    "    reward = R[action][s]\n",
    "    sum = 0\n",
    "    for s_prime in range(len(S)):\n",
    "        sum += T[action][s][s_prime] * U[s_prime]\n",
    "    sum *= gamma\n",
    "    return (reward + sum)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_sa(s, a, U, pi, gamma, T=T, R=R):\n",
    "    action = a\n",
    "    reward = R[action][s]\n",
    "    sum = 0\n",
    "    for s_prime in range(len(states)):\n",
    "        sum += T[a][s][s_prime] * U[s_prime]\n",
    "    sum *= gamma\n",
    "    return (reward + sum)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synchronous Value Iteration BUT SHOULD BE MCTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = {0:0, 1:0, 2:0, 3:0} # initial policy\n",
    "U = np.array([0.0 for _ in range(len(states))]) # initial values\n",
    "\n",
    "# for implementation\n",
    "U_vals = [[u] for u in U] # store U values\n",
    "old_U = U - 0.1 # to start loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: {0: 2, 1: 1, 2: 0, 3: 1}\n",
      "Total Iterations: 138\n"
     ]
    }
   ],
   "source": [
    "# iterate until values converge within 0.01\n",
    "iterations = 0\n",
    "while np.sum(U-old_U) > 0.01:\n",
    "    iterations += 1\n",
    "    Q = {}\n",
    "    new_pi = {}\n",
    "    # update policy based on U values\n",
    "    for s in range(len(states)): # iterate over all states\n",
    "        argmax_a = None\n",
    "        max_Q = -1*float('inf')\n",
    "        \n",
    "        for a in range(len(T)): # iterate over possible actions\n",
    "            Q[(s,a)] = Q_sa(s, a, U, pi, gamma) # calculate Q-value\n",
    "            if Q[(s,a)] > max_Q: # get max, argmax\n",
    "                argmax_a = a\n",
    "                max_Q = Q[(s,a)]\n",
    "        new_pi[s] = argmax_a # update policy for state\n",
    "    pi = new_pi # update full policy\n",
    "    \n",
    "    # update values\n",
    "    old_U = deepcopy(U) # keep old value to determine convergence\n",
    "    U = [bellman_backup(i, U, pi, gamma) for i in range(len(states))]\n",
    "    U = np.array(U)\n",
    "    \n",
    "    # for plotting\n",
    "    for i, u in enumerate(U):\n",
    "        U_vals[i].append(u)\n",
    "\n",
    "U_value_iteration = U\n",
    "print(\"Policy:\", pi)\n",
    "print(\"Total Iterations:\", iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = {0:0, 1:0, 2:0, 3:0} # initial policy\n",
    "old_pi = None\n",
    "U = np.array([0.0 for _ in range(len(states))]) # initial values\n",
    "\n",
    "# for implementation\n",
    "U_vals = [[u] for u in U] # store U values\n",
    "old_U = U - 0.1 # to start loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Policy: {0: 2, 1: 1, 2: 0, 3: 1}\n",
      "Total Iterations: 135\n",
      "Policy: {0: 2, 1: 1, 2: 0, 3: 1}\n",
      "Total Iterations: 234\n"
     ]
    }
   ],
   "source": [
    "print(pi)\n",
    "iterations = 0\n",
    "while pi != old_pi:\n",
    "    # determine U of policy\n",
    "    # iterate until values converge within 0.01\n",
    "    while np.sum(U-old_U) > 0.01:\n",
    "        iterations += 1\n",
    "        old_U = deepcopy(U)\n",
    "        U = np.array([bellman_backup(i, U, pi, gamma) for i in range(len(states))])\n",
    "        # for plotting\n",
    "        for i, u in enumerate(U):\n",
    "            U_vals[i].append(u)\n",
    "    \n",
    "    # extract policy from U values\n",
    "    Q = {}\n",
    "    new_pi = {}\n",
    "    for s in range(len(states)): # iterate over all states\n",
    "        argmax_a = None\n",
    "        max_Q = -1*float('inf')\n",
    "\n",
    "        for a in range(len(T)): # iterate over possible actions\n",
    "            Q[(s,a)] = Q_sa(s, a, U, pi, gamma) # calculate Q-value\n",
    "            if Q[(s,a)] > max_Q: # get max, argmax\n",
    "                argmax_a = a\n",
    "                max_Q = Q[(s,a)]\n",
    "        new_pi[s] = argmax_a # update policy for state\n",
    "    old_pi = deepcopy(pi)\n",
    "    pi = new_pi # update full policy\n",
    "    U = [bellman_backup(i, U, pi, gamma) for i in range(len(states))]\n",
    "    U = np.array(U)\n",
    "    print(\"Policy:\", pi)\n",
    "    print(\"Total Iterations:\", iterations)\n",
    "\n",
    "U_policy_iteration = U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing stuff now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53.13439528 56.00000181 57.27536126 65.07537912]\n",
      "[53.13694775 56.00255428 57.27791374 65.07793159]\n"
     ]
    }
   ],
   "source": [
    "print(U_value_iteration)\n",
    "print(U_policy_iteration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
